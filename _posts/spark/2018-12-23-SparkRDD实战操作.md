---
title:  "SparkRDD实战操作"
layout: archive
date:   2018-12-23 08:44:13
categories: spark
---

### Transform 

| 转换                             | 含义                                                         |
| -------------------------------- | ------------------------------------------------------------ |
| map(func)                        | 返回一个新的RDD，该RDD由每一个输入元素经过func函数后转换而成 |
| flatMap(func)                    | 与map类似，但是第一个输入可以被映射为0个或多个输出(func应该返回的是一个序列) |
| filter(func)                     | 过滤，根据fun中给定的条件，返回符合条件的元素                |
| union(otherDataset)              | 对于源RDD和参数RDD求**并集**返回一个新RDD,不去重             |
| intersection(otherDataSet）      | 交集                                                         |
| join(otherDataset)               | 关联查询，(K,V) (K,W) => (K, (V,W)), 应用场景： One to One   |
| cogroup                          | 关联查询，(K,V) (K,W) => (K,Iterable[V],Iterable[W]) one to many |
| groupByKey([numTasks])           | 在一个(K,V)的RDD上调用，返回一个（V,Iterator(V)）的RDD       |
| reduceByKey(func, [numTasks])    | 在一个(K, V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将key聚合到一起与 |
| sortByKey([ascending],[numTask]) | 在一个(K,V)的RDD上调用，K必须实现Order接口，返回一个按照Key进行排序的RDD |
| aggregateByKey()                 | 根据Key进行聚合                                              |
| distinct([numTasks])             | 对于源RDD去重后返回一个新的RDD                               |

### Action

| 动作                 | 含义                                                         |
| -------------------- | ------------------------------------------------------------ |
| reduce(func)         | 通过func函数聚集RDD中的所有元素，                            |
| foreach              | 没有返回值                                                   |
| count                | 返回RDD元素个数                                              |
| countByKey           | 针对 (K, V)类型的RDD，返回一个(K, Int)的map, 表示每一个Key对应的元素 |
| collect              | 在驱动程序中，以数组形式返回                                 |
| first                | 返回RDD的第一个元素（类似take(1)）                           |
| take(n)              | 返回一个由数据集的前n个元素组成的数组                        |
| saveAsTextFile(Path) | 将文件按路径存储为textFile格式                               |

### Transfer操作实战

1. map

   ```scala
   // 求平方
   val conf = new SparkConf().setAppName("map").setMaster("local")
   val sc = new SparkContext(conf)
   
   val numbers = Array(1, 2, 3, 4, 5, 6)
   val numRDD = sc. parallelize(numbers)
   val resultRDD = numRDD.map( num => num*num )
   resultRDD.foreach(println)
   sc.stop()
   ```

2. filer 

   ```scala
   // 偶数
   val conf = new SparkConf().setAppName("map").setMaster("local")
   val sc = new SparkContext(conf)
   
   val numbers = Array(1, 2, 3, 4, 5, 6)
   val numRDD = sc. parallelize(numbers)
   val resultRDD = numRDD.filter(_ % 2 = 0)
   resultRDD.foreach(println)
   sc.stop()
   ```

3. flatMap

   ```scala
   // 将单词进行拆分 
   val conf = new SparkConf().setAppName("flatMap").setMaster("local")
   val sc = new SparkContext(conf)
   
   val words = Array("hello python", "hello spark", "hello scala")
   val wordRDD = sc. parallelize(words)
   wordRDD.flatMap(._split(" ")).foreach(println)
   sc.stop()
   ```

4. groupByKey

   ```scala
   // 将每个班成绩进行分组
   val conf = new SparkConf().setAppName("groupByKey").setMaster("local")
   val sc = new SparkContext(conf)
   
   val arr = Array(Tuple2("class1", 80),Tuple2("class2", 75),Tuple2("class3", 90))
   val groupByKeyRDD = sc.parallelize(arr).groupByKey
   groupByKeyRDD.foreach(println)
   sc.stop()
   ```

5. reduceByKey

   ```scala
   // 统计每个班级的总分
   val conf = new SparkConf().setAppName("reduceByKey").setMaster("local")
   val sc = new SparkContext(conf)
   
   val arr = Array(Tuple2("class1", 80),Tuple2("class2", 75),Tuple2("class3", 90))
   val reduceByKeyRDD = sc.parallelize(arr).reduceByKey(_+_)
   reduceByKeyRDD.foreach(println)
   sc.stop()
   ```

6. sortByKey

   ```scala
   // 将学生按分数排序
   val conf = new SparkConf().setAppName("sortByKey").setMaster("local")
   val sc = new SparkContext(conf)
   
   val arr = Array(Tuple2("max", 80),Tuple2("mike", 98),Tuple2("bob", 70))
   // sortByKey 是将K,V 形式的数据 ，按key 进行排序
   // ascending 为Flase则为降序
   val reduceByKeyRDD = sc.parallelize(arr).map(x =>(x._2, x._1))
   	.sortByKey(ascending = false).map(x => (x._2, x._1))
   reduceByKeyRDD.foreach(println)
   sc.stop()
   ```

7. join

   ```scala
   //打印每个学生成绩
   val conf = new SparkConf()
         .setAppName("join")
         .setMaster("local")
   val sc = new SparkContext(conf)
   
   val arr1 = Array(Tuple2("1001", "Mike"), Tuple2("1002", "Bob"))
   val arr2 = Array(Tuple2("1001", 38),Tuple2("1002", 24))
   val rdd1 = sc.parallelize(arr1)
   val rdd2 = sc.parallelize(arr2)
   val joinRDD = rdd1.join(rdd2)
   joinRDD.foreach(x => {
       println("id:"+x._1+" name:"+x._2._1+" score:"+x._2._2)
   })
   sc.stop()
   ```

8. cogroup

   ```scala
   // 打印每个学生成绩
   val conf = new SparkConf()
         .setAppName("coGroupRDD")
         .setMaster("local")
   val sc = new SparkContext(conf)
   
   val arr1 = Array(("class1", "Mike"), ("class1", "Bob"), ("class2", "max"))
   val arr2 = Array(("class1", 38), ("class1", 24), ("class2", 50))
   
   val rdd1 = sc.parallelize(arr1)
   val rdd2 = sc.parallelize(arr2)
   val cogroupRDD = rdd1.cogroup(rdd2)
   
   cogroupRDD.foreach(x => {
       println(x._1)
       for(i <- x._2._1) {
           println(i)
       }
   
       for(j <- x._2._2) {
           println(j)
       }
   })u
   ```

9. union

   ```scala
   // 将两个RDD合并 不去重
   val conf = new SparkConf().setAppName("union").setMaster("local")
   val sc = new SparkContext(conf)
   
   val rdd1 = sc.parallelize( 1 to 10)
   val rdd2 = sc.parallelize( 9 to 20)
   
   val unionRDD = rdd1.union(rdd2)
   unionRDD.foreach(println)
   sc.stop()
   ```

10. instersection

    ```scala
    // 取两个RDD的交集并去重
    val conf = new SparkConf().setAppName("intersection").setMaster("local")
    val sc = new SparkContext(conf)
    
    val rdd1 = sc.parallelize(1 to 10)
    val rdd2 = sc.parallelize(5 to 15)
    val intersectionRDD = rdd1.intersection(rdd2)
    
    intersectionRDD.foreach(println)
    sc.stop()
    ```

11. distinct

    ```scala
    // 去重操作
    val conf = new SparkConf().setAppName("distinct").setMaster("local")
    val sc = new SparkContext(conf)
    val arr1 = Array(Tuple3("max", "math", 90), Tuple3("max", "english", 75),
          Tuple3("max", "math", 100), Tuple3("Bob", "math", 60))
    val rdd = sc.parallelize(arr1)
    val users = rdd.map(_._1).distinct.collect
    println(users.mkString(","))
    ```

12. aggregateByKey

    ```
    
    ```


### Action操作实战

