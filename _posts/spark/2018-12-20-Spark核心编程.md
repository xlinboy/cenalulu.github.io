---
title:  "Spark核心编程"
layout: archive
date:   2018-12-20 08:44:13
categories: spark
---

### 1. 什么是Spark

> Spark 以前是一个单一的内存计算机框架，发展至今成为一个独立的生态系统 
>
> 现在基本每年都会在北京，深圳开设一些关于Spark的相关圆桌会议
>
> 相对于Spark来说，有一个相似的计算机框架，Storm。Storm是一个纯粹的实时计算框架
>
> Spark支持离线计算，通过SQL的方式进行数据处理、分析
>
> 此外也具备Streaming的准实时计算机方式。

- 是一个由Scala编写的、支持复杂运算的、基于内存的、准实时的大数据计算框架
- 同时也支持离线计算机
- 通过SQL的方式进行数据处理、分析
- 具备Streaming的准实时计算机方式

Spark能替代Hadoop吗？

> 不能， Spark不能代替Hadoop的主要原因，不是在于技术框架， 而是在于业务场景。

Spark很快？（不算Streaming）

> 相对于Hadoop， Hadoop跑1时间，Spark10分钟、 Hadoop跑1天，Spark跑1小时

### Spark集群概述

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/cluster-overview.png)

**角色介绍**

1. Dirver Program
   - 编写的Spark程序
2. SparkContext
   - 在每个Driver Program中都会存在一个SparkContext
   - SparkContext主要用于跟Spark集群进行交互（申请计算资源，配置我们任务调度机制...）
3. Cluster Manager
   - 管理Spark集群以及与Driver Program中的SparkContext进行交互
4. Worker Node
   - 就是一台Spark集群中的计算机节点，通常与DataNode是同一台（数据本地化）
   - 每台Worker都具有一定的Cpu Cores和Memory
5. Executor
   - Executor是一个进程，是Spark集群中，资源分配的最小单元
   - 一个Driver Program 会拥有多个Executor
   - 一个Worker Node下会产生多个Executor
   - 但Executor本身只做任务的管理与调度
6. Task
   - Task是一个线程， Driver Program中的每一个RDD操作都会转换成一个Task任务
   - Driver Program会将一批Task发送到Excutor中
   - 由Excutor来管理执行Task，将结果反馈给SparkContext中

### 2.  什么是RDD

#### 2.1 RDD（弹性分布式数据集）

1. 不可变
2. 只读
3. 具有自动容错恢复机制（DAG，有向无环图）
4. 具有优先选择数据处理节点位置的机制（计算本地化）
5. 本身并不是数据集，而一个并行处理数据集的工具

#### 2.2 内部每个RDD有以下主要的五个属性

- 分区列表（集合）
- 第一个分片，都具备 一个计算函数
- 每个RDD都有一个RDD依赖列表，用于找到当前RDD转换所需要的其他RDD
- 可选的，针对 KV类型的RDD分区器，例如：`HashPartitioner` ， `RangePartitioner`
- 可选的，对于每一个的分片的优先计算机位置列表，例如从HDFS中读取文件，每个分区默认对应一个Block

#### 2.3 RDD两种数据集创建方式

1. 并行化集合
   - 参考主机的`cpu core`的数量
   - 划分的分区数一般是` cpu core`的两倍

2. 外部数据集，例如`HDFS`，`AWS` , `S3`
   - 默认情况下，`HDFS`上文件对应的`BLOCK`数量有多少，`Spark`就有多少`Partition`
   - 可以大于当前`BLOCK`数量 的`partition`，但最低不能低于`HDFS`上的`block`数量。

#### 2.4 RDD动作

1. Transformation
   - 主要是多个RDD之间数据的转换 (Lazy)
2. Action
   - 主要的用于最终的数据聚合，数据转换后的计算结果
   - 每次调用一个Action操作时， 每一个RDD都会重新被计算

只有在执行Action时，才会触发Spark的任务调度与计算， Transformation是一个Lazy操作。

- 可以通过将RDD持久化到内存中的方式，来避免RDD被重复计算。
- 可以使用persist 或者 cache方法（cache底层调用的就是persist）

#### 2.5 RDD 缓存、持久化（persist）

>  Spark非常快速的原因之一，就是在不同的操作中在内存中持久化一个数据集。
>
> 当持久化一个数据集后，每个节点都将把计算的分片结果，保存在内存中，并在此数据集后面进行的(action)操作，会对其进行重用。这使用后续动作变得更加迅速（通常快10倍）。

假设首先进行`RDD0 -> RDD1 -> RDD2`的计算作业，那么计算结束时，`RDD1`就已经缓存在系统中。在进行`RDD0 -> RDD1 -> RDD3`的作用时，由于`RDD1`已经缓存在系统中，只需进行`RDD1 -> RDD3`，因此计算速度可以得到很大提升。

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/spark_partition_cach.png)

##### 两种持久化方法

- persist
- cache(底层调用persist方法)

##### Spark持久化级别

| 持久化级别          | 含义解释                                                     |
| ------------------- | ------------------------------------------------------------ |
| MEMORY_ONLY         | 默认级别，如果cache中的元素无法全部缓存到内存中，则丢失一部分。 有一部分数据还是会重新计算（数据无法完全存储到内存中）。**数据没有经过序列化** |
| MEMORY_AND_DISK     | 优先将数据存储到内存中。如果内存不够，就将数据存储到磁盘。   |
| MEMORY_ONLY_SER     | 为了节省更多空间，我们可以先数据序列化，然后在存储在内存中。推荐使用**Kryo serialization**进行序列化。 |
| MEMORY_AND_DISK_SER | 基本含义与MEMORY_AND_DISK相同。唯一的区别是， 会将RDD的数据进行序列化。 |
| DISK_ONLY           | 使用未序列化Java对象格式，将数据存储到磁盘文件中             |
| MEMORY_ONLY_2       | cache的数据会具有多少个副本                                  |

##### 这么多持久化，哪家强（应该使用哪种）?

> Spark的优势在于内存，如果还向磁盘上写入数据，那么会成为Spark版的Hadoop。
>
> 不要往磁盘上进行Cache上，磁盘的读写速度还不如在内存中重新做计算。

#### 缓存的使用场景?

1. 获取大量数据之后，比如来自外部数据源
2. 进行一个非常长的计算链条
3. 某个计算非常耗时
4. 进行`checkpoint`之前

##### 如何选择持久化策略?

1. 优先使用`MEMORY_ONLY`

2. 内存放不下就使用`MEMORY_ONLY_SER`，但会额外消耗CPU资源

3. 容错恢复使用`MEMORY_ONLY_2 `
4. 尽量不适用`DISK`相关策略

#### 2.6 RDD Checkpoint(检查点)

> Checkpoint 的引入： 如果缓存丢失了，则需要重新计算。但有的时候，计算特别复杂或计算特别耗时，那么缓存丢失对于整个Job是影响是非常大的。
>
> Checkpoint机制：计算完成后，重新建立一个Job来计算。为了避免重复计算，推荐先将RDD进行缓存。

#### 2.7 Shared Variables

1. `Broadcast Variables`(广播变量)
   - 广播变量就是一个全局只读的变量或集合。	
   - 因为我们没有必要把每一个相关变量都往我们的Executor进行拷贝
2. `Accumulators`(累加器)
   - 定义个全局的累加变量(只能做累加)，用于所有的Executor执行全局累加操作		

### 3 RDD 的流程和DAG

##### WordCount 流程

```scala
val conf = new SparkConf().setAppName("WordCount").setMaster("local")
val sc = new SparkContext(conf)
val textFile = sc.textFile("C:\\Users\\13194\\Desktop\\test.txt")
val wordsRdd = textFile.flatMap(_.split(" "))
val wordsCountRdd = wordsRdd.map((_, 1)).reduceByKey(_+_)
wordsCountRdd.collect().foreach(println)
```

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/WordCount%E6%B5%81%E7%A8%8B.png)

- Action主要是为了触发Spark的计算任务，每一个Transform都会转成一个Task任务执行，当到Shuffle Task时，会进行Shuffle
- 整个计划流程根据具体的流程划分不同的Stage（不同的执行部分）
- 优先使用reduceByKey来代替groupByKey（优化）
  - 使用ReduceByKey会有一个本地优化，在本地将数据进行合并。

#### RDD 依赖关系

- 窄依赖(narrow dependency)：每一个parent RDD的partition最多被子RDD的一个Partition使用
- 宽依赖(wide dependency)：多个子RDD的Partition会依赖同一个parent RDD的Partition

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/spark_rdd_dependency.png)

##### RDD Lineages(生命线)

> RDD也是一个DAG的任务集合，一个DAG代表了一个RDD的计算过程。
>
> 每个DAG都会记住创建该数据集需要哪些操作，跟踪记录RDD的继承关系。

##### Transfermations 常见的窄依赖

- `map` 、`flatMap`、`filter`、`mapParitions`、`union`

> 它们只是将Parition的数据根据规则进行转化，并不做其它处理
>
> 只是将数据从一个形式转化成另一种形式

##### Transfermations 常见的宽依赖, 

- `cogroup`、`join`、`gourpByKey`、`reduceByKey`、`distinct`、`intersection`

> 对于groupByKey 子RDD所有Partition会依赖于 parent RDD的所有Partition
>
> 子RDD的Partition是parent RDD的所有Partition Shuffle的结果

##### 怎么分辨 `Narrow Or Wide Dependency`

- Narrow dependency objects
  - `OneToOneDependency`
  - `PruneDependency`
  - `RangeDependency ` 范围依赖
- Wide denpendency objects
  - `ShuffleDependency`

- 查看RDD依赖关系 `option.dependenices`

- 查看RDD生命线 `option.toDebugString`

##### RDD容错机制（DAG）

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/2018-12-20%2010-47-56.JPG)

> 如果某个RDD计算错误，子RDD则找其父RDD，重新计算该RDD，如果此时是宽依赖，则会对集群性能造成很大影响。 
>
> 此时解决的办法，是将数据持久化。
>
> 持久化两种机制：	
>
> 1. 内存持久（cache)
> 2. 检查点（Checkpoint, 磁盘持久化）

#### Understanding closures （理解闭包函数）

```scala
var counter = 0
val data = Array(1, 2, 3, 4, 5, 6)
var rdd = sc.parallelize(data)
// Wrong: Don't do this!!
rdd.foreach(x => counter += x)
// 此时输出的结果为0
println("Counter value: " + counter)
```

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/spark_closures%20.png)

- 每一个RDD操作会在Executor中初始化成一个Task任务。

- 发送到集群中每个Executor 的是`counter`的副本，而不是Driver Program 中的`var counter = 0`
- 每个Executor中任有`counter`进行计算，但这些操作对Driver Program 来说不存在。
- 所以最终输出`counter`的结果为`0`

此时，可以用全局变量解决该问题：

```scala
val accum = sc.accumulator(0, "My Accumulator")
sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
accum.value
```

### Shuffle operations

- Shuffle 是什么

> 不同节点之间的数据传输

- Shuffle的缺点

>在Spark中的某些RDD操作，会触发Shuffle事件。
>
>Shuffle事件一旦产生，就把我们所操作的数据集进行重新分布。
>
>一旦涉及到重新分布，将会耗费大量的IO与网路资源。

##### ReduceByKey 生产中会遇到的问题

- 需求：

> 第一列：代表游客的编号
> 第二列：代表游客旅行地
> 第三列：代表游客旅行的消费
> //统计出每一个用户本月旅行次数以及总消费
> (100,2,300)

- 代码如下：

```scala
 val arr2 = Array(
      (100,"Geneva",22.25),
      (100,"Zurich",42.10),
      (200,"Fribourg",12.40),
      (200,"St.Gallen",8.20),
      (300,"Lucerne",31.60),
      (300,"Basel",16.20)
    )
val rdd = sc.parallelize(arr2)
println("次数：" + groupByKeyRDD.count)
groupByKeyRDD.foreach(x => {
    println("ID: " + x._1)
    println("总额：" + x._2.sum)
})
```

此时就会产生Shuffer：

<img src="https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/Spark_group_shuffle.png" style="height:450px; width:700px;" >

在进行Shuffle操作前，尽可能的减少每个分区即将进行Shuffle的数据量。(优化技巧)

- 数据倾斜。

我现在有上亿条的数据需要进行分组求和、求平均，该如何操作？为什么？

> 解决方案就是，在执行Shuffle前，尽可能的减少需要交换的数据。
>
> 减少Shuffle数据的两种方案？
>
> 1. 选择具有Combiner函数的RDD操作（reduceByKey）
>
> 2. 对业务数据进行数据变换.
>
>    ```scala
>    (100,"xxxx",2500)
>    (100,(1,2500))
>    (2500,(1,100))
>    ```

Shuffle 优化主要目的：

1. 数据处理（例如：reduceByKey 本地shuffle提速）
2. RDD容错计算（提速）

#### Spark 经验之谈

如果面对比较复杂数据分析时，通常会将数据转成Tuple

Tuple操作的好处： 列式操作

```scala
("class1", 85)
("class1", 100)
("class2", 85)
("class2", 100)
```

