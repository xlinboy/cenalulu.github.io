---
title:  "Spark核心编程"
layout: archive
date:   2018-12-20 08:44:13
permalink: /spark/
categories: spark
---

### 1. 什么是Spark

> Spark 以前是一个单一的内存计算机框架，发展至今成为一个独立的生态系统 
>
> 现在基本每年都会在北京，深圳开设一些关于Spark的相关圆桌会议
>
> 相对于Spark来说，有一个相似的计算机框架，Storm。Storm是一个纯粹的实时计算框架
>
> Spark支持离线计算，通过SQL的方式进行数据处理、分析
>
> 此外也具备Streaming的准实时计算机方式。

- 是一个由Scala编写的、支持复杂运算的、基于内存的、准实时的大数据计算框架
- 同时也支持离线计算机
- 通过SQL的方式进行数据处理、分析
- 具备Streaming的准实时计算机方式。

### 2. Spark RDD核心编程

#### RDD（弹性分布式数据集）

1. 不可变
2. 只读
3. 具有自动容错恢复机制（DAG，有向无环图）
4. 具有优先选择数据处理节点位置的机制（计算本地化）
5. 本身并不是数据集，而一个并行处理数据集的工具

#### 内部每个RDD有以下主要的五个属性

- 分区列表
- 第一个分片，都具备 一个计算函数
- 每个RDD都有一个RDD依赖列表，用于找到当前RDD转换所需要的其他RDD
- 可选的，针对 KV类型的RDD分区器，例如：`HashPartitioner` ， `RangePartitioner`
- 可选的，对于每一个的分片的优先计算机位置列表，例如从HDFS中读取文件，每个分区默认对应一个Block

#### 两个数据集创建方式

1. 并行化集合
   - 参考主机的`cpu core`的数量
   - 划分的分区数一般是` cpu core`的两倍

2. 外部数据集，例如`HDFS`，`AWS` , `S3`
   - 默认情况下，`HDFS`上文件对应的`BLOCK`数量有多少，`Spark`就有多少`Partition`
   - 可以大于当前`BLOCK`数量 的`partition`，但最低不能低于`HDFS`上的`block`数量。

#### RDD的两种操作

1. Transformation
   - 主要是多个RDD之间数据的转换
2. Action
   - 主要的用于最终的数据聚合

只有在执行Action时，才会触发Spark的任务调度与计算， Transformation是一个Lazy操作。

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/WordCount%E6%B5%81%E7%A8%8B.png)

- Action主要是为了触发Spark的计算任务，每一个Transform都会转成一个Task任务执行，当到Shuffle Task时，会进行Shuffle

- 整个计划流程根据具体的流程划分不同的Stage（不同的执行部分）

- 优先使用reduceByKey来代替groupByKey（优化）