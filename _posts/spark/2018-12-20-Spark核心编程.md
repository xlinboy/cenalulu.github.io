---
title:  "Spark核心编程"
layout: archive
date:   2018-12-20 08:44:13
permalink: /spark/
categories: spark
---

### 1. 什么是Spark

> Spark 以前是一个单一的内存计算机框架，发展至今成为一个独立的生态系统 
>
> 现在基本每年都会在北京，深圳开设一些关于Spark的相关圆桌会议
>
> 相对于Spark来说，有一个相似的计算机框架，Storm。Storm是一个纯粹的实时计算框架
>
> Spark支持离线计算，通过SQL的方式进行数据处理、分析
>
> 此外也具备Streaming的准实时计算机方式。

- 是一个由Scala编写的、支持复杂运算的、基于内存的、准实时的大数据计算框架
- 同时也支持离线计算机
- 通过SQL的方式进行数据处理、分析
- 具备Streaming的准实时计算机方式

Spark能替代Hadoop吗？

> 不能， Spark不能代替Hadoop的主要原因，不是在于技术框架， 而是在于业务场景。

Spark很快？（不算Streaming）

> 相对于Hadoop， Hadoop跑1时间，Spark10分钟、 Hadoop跑1天，Spark跑1小时

### 2. Spark RDD核心编程

#### RDD（弹性分布式数据集）

1. 不可变
2. 只读
3. 具有自动容错恢复机制（DAG，有向无环图）
4. 具有优先选择数据处理节点位置的机制（计算本地化）
5. 本身并不是数据集，而一个并行处理数据集的工具

#### 内部每个RDD有以下主要的五个属性

- 分区列表
- 第一个分片，都具备 一个计算函数
- 每个RDD都有一个RDD依赖列表，用于找到当前RDD转换所需要的其他RDD
- 可选的，针对 KV类型的RDD分区器，例如：`HashPartitioner` ， `RangePartitioner`
- 可选的，对于每一个的分片的优先计算机位置列表，例如从HDFS中读取文件，每个分区默认对应一个Block

#### 两个数据集创建方式

1. 并行化集合
   - 参考主机的`cpu core`的数量
   - 划分的分区数一般是` cpu core`的两倍

2. 外部数据集，例如`HDFS`，`AWS` , `S3`
   - 默认情况下，`HDFS`上文件对应的`BLOCK`数量有多少，`Spark`就有多少`Partition`
   - 可以大于当前`BLOCK`数量 的`partition`，但最低不能低于`HDFS`上的`block`数量。

#### RDD的两种操作

1. Transformation
   - 主要是多个RDD之间数据的转换
2. Action
   - 主要的用于最终的数据聚合

只有在执行Action时，才会触发Spark的任务调度与计算， Transformation是一个Lazy操作。

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/WordCount%E6%B5%81%E7%A8%8B.png)

- Action主要是为了触发Spark的计算任务，每一个Transform都会转成一个Task任务执行，当到Shuffle Task时，会进行Shuffle
- 整个计划流程根据具体的流程划分不同的Stage（不同的执行部分）
- 优先使用reduceByKey来代替groupByKey（优化）
  - 使用ReduceByKey会有一个本地优化，在本地将数据进行合并。

#### Spark RDD容错机制（DAG）

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/2018-12-20%2010-47-56.JPG)

- 窄依赖。表示父RDD的每一个分区对应一个子分区RDD。
- 宽依赖。表示子RDD的分区依赖于父RDD的部分或所有分区。

> 如果某个RDD计算错误，子RDD则找其父RDD，重新计算该RDD，如果此时是宽依赖，则会对集群性能造成很大影响。 
>
> 此时解决的办法，是将数据持久化到本地。

持久化两种机制：	

1. 内存持久（cache)
2. 检查点（Checkpoint, 磁盘持久化）

## Spark集群概述

![img](https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/cluster-overview.png)

**角色介绍**

1. Dirver Program
   -  编写的Spark程序
2. SparkContext
   - 在每个Driver Program中都会存在一个SparkContext
   - SparkContext主要用于跟Spark集群进行交互（申请计算资源，配置我们任务调度机制...）
3. Cluster Manager
   - 管理Spark集群以及与Driver Program中的SparkContext进行交互
4. Worker Node
   - 就是一台Spark集群中的计算机节点，通常与DataNode是同一台（数据本地化）
   - 每台Worker都具有一定的Cpu Cores和Memory
5. Executor
   - Executor是一个进程，是Spark集群中，资源分配的最小单元
   - 一个Driver Program 会拥有多个Executor
   - 一个Worker Node下会产生多个Executor
   - 但Executor本身只做任务的管理与调度
6. Task
   - Task是一个线程， Driver Program中的每一个RDD操作都会转换成一个Task任务
   - Driver Program会将一批Task发送到Excutor中
   - 由Excutor来管理执行Task，将结果反馈给SparkContext中

#### ReduceByKey 生产中会遇到的问题

需求：

> 第一列：代表游客的编号
> 第二列：代表游客旅行地
> 第三列：代表游客旅行的消费
> //统计出每一个用户本月旅行次数以及总消费
> (100,2,300)

代码如下：

```scala
 val arr2 = Array(
      (100,"Geneva",22.25),
      (100,"Zurich",42.10),
      (200,"Fribourg",12.40),
      (200,"St.Gallen",8.20),
      (300,"Lucerne",31.60),
      (300,"Basel",16.20)
    )
val rdd = sc.parallelize(arr2)
println("次数：" + groupByKeyRDD.count)
groupByKeyRDD.foreach(x => {
    println("ID: " + x._1)
    println("总额：" + x._2.sum)
})
```

此时就会产生Buffer：

<img src="https://xlactive-1258062314.cos.ap-chengdu.myqcloud.com/Spark_group_shuffle.png" style="height:500px; width:800px;" >

### Shuffle operations

Shuffle 是什么

> 不同节点之间的数据传输

Shuffle 优化主要目的：

1. 数据处理（例如：reduceByKey 本地shuffle提速）
2. RDD容错计算（提速）

#### Spark 经验之谈

如果面对比较复杂数据分析时，通常会将数据转成Tuple

Tuple操作的好处： 列式操作

```scala
("class1", 85)
("class1", 100)
("class2", 85)
("class2", 100)
```



**查看依赖关系** `option.dependenices`

**查看RDD生命线**`option.toDebugString`

