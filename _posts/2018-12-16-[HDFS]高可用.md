---
layout: post
title: '[HDFS]高可用'
subtitle: 'Hadoop高可用的的配置及起因'
date: 2018-12-16
categories: HDFS
cover: '../../../assets/img/hadoop-logo.jpg'
tags: HDFS
---

>  它需要zookeeper的调度，首先配置zookeeper

#### HDFS HA（热备、不需要secondarynamenode）  
- namenode active 主要的，活动的
- namenode standby(backup) 备用状态

#### 问题起因：
1. namenode宕机，这段时间所有元数据丢失，hdfs无法提供服务。  
   - `SPOF（single point of failure）`单点故障即会整体故障。

2. namenode节点，有一些服务需要升级，也需要停止服务才能升级

#### 解决方法
1. 操作日志信息给到journalnode管理（确保两个namenode都能同步更新edits）
2. 配置两个namenode
3. 客户端的访问实际地址，会自动分配到active的namenode
4. 两个namenode要隔离，同一时刻只有一个namenode对外提供服务


#### 配置信息

##### etc/hadoop/hdfs-site.xml
```
<property>
	# 对整个文件系统需要一个统称
	<name>dfs.nameservices</name>
	<value>ns1</value>
</property>

<property>
	# 指明这个文件系统的namenode有哪些
	<name>dfs.ha.namenodes.ns1</name>
	<value>nn1,nn2</value>
</property>

<property>
	# 指明nn1是哪个
	<name>dfs.namenode.rpc-address.ns1.nn1</name>
	<value>hadoop1:8020</value>
</property>

<property>
	# 指明nn2是那个
	<name>dfs.namenode.rpc-address.ns1.nn2</name>
	<value>hadoop2:8020</value>
</property>

<property>
	# 指明nn1访问地址端口
	<name>dfs.namenode.http-address.ns1.nn1</name>
	<value>hadoop1:50070</value>
</property>
<property>
	# 指明nn2访问地址端口
	<name>dfs.namenode.http-address.ns1.nn2</name>
	<value>hadoop2:50070</value>
</property>

<property>
	# 共享日志在journalnode上的共享端口
	<name>dfs.namenode.shared.edits.dir</name>
	<value>qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/ns1</value>
</property>
# 配置edits在journalnode上的保存地址
<property>
	<name>dfs.journalnode.edits.dir</name>
	# 注意这里的版本信息
	<value>/opt/programs/hadoop-2.7.7/data/dfs/jn</value>
</property>

<property>
	# 配置proxy代理客户端
	<name>dfs.client.failover.proxy.provider.ns1</name>
	<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
```

##### etc/hadoop/core-site.xml

```
# 配置两个namenode的隔离策略
# sshfence方式
# 使用这种方式，必须实现ssh无密码登陆

<property>
	<name>dfs.ha.fencing.methods</name>
	<value>sshfence</value>
</property>

<property>
	<name>dfs.ha.fencing.ssh.private-key-files</name>
	<value>/root/.ssh/id_rsa</value>
</property>

<property>
    <name>fs.defaultFS</name>
    <value>hdfs://ns1</value>
</property>

```

##### 最小化安装CentOS版本，需要在每一个节点都手动安装fence组件psmisc
```
yum -y install psmisc  
```

> psmisc包含fuser，killall，pstree三个程序，且出现上述问题是由于我们在安装centos7的时候选择了最小化安装，默认是不安装psmics。  
fuser 显示使用指定文件或者文件系统的进程的PID。  
killall 杀死某个名字的进程，它向运行指定命令的所有进程发出信号。  
pstree 树型显示当前运行的进程。


#### 启动集群：

关闭所有守护进程，清除之前残留的data/tmp/*信息

```
1.启动所有的journalnode
    sbin/hadoop-daemon.sh start journalnode
2.nn1格式化并启动
    bin/hdfs namenode -format
    sbin/hadoop-daemon.sh start namenode
3.在nn2上，同步nn1的元数据信息
    bin/hdfs namenode -bootstrapStandby
4.启动nn2
    sbin/hadoop-daemon.sh start namenode
5.启动datanode节点
    sbin/hadoop-daemon.sh start datanode
6.把节点设置为active
    bin/hdfs haadmin -transitionToActive nn1
```

### 自动故障转移，借助于zookeeper
1. 启动时都是standby，选举一个为active
2. 监控 ZKFC, 给每一个namenode都增加一个ZKFC服务。

#### 配置

hdfs-site.xml:增加。 打开自动故障转移
```
<property>
	<name>dfs.ha.automatic-failover.enabled</name>
	<value>true</value>
</property>
```

core-site.xml: 添加zookeeper的服务
```
<property>
	<name>ha.zookeeper.quorum</name>
	<value>hadoop1:2181,hadoop2:2181,hadoop3:2181</value>
</property>
```

#### 启动
1. 关闭所有hdfs服务，scp同步修改的文件
2. 启动zookeeper脚本，每个节点都启动. `bin/zkServer start`
2. 启动zookeeper集群， `sbin/hadoop-daemon.sh start journalnode`
3. 初始化HA在zookeeper中的状态

	`bin/hdfs zkfc -formatZK -force`
4. 启动hdfs服务。如果直接一起启动出现通信错误，造成namenode停止，则**需要先启动journalnode，再启动其他**

5. namenode节点启动zkfc服务。先在哪台节点启动zkfc，那么哪台就是active

   `sbin/hadoop-daemon.sh start zkfc`

