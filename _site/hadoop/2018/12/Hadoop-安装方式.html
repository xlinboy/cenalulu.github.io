<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[Hadoop]安装方式 - xlin的个人博客</title>
    <meta name="author"  content="xlin">
    <meta name="description" content="[Hadoop]安装方式">
    <meta name="keywords"  content="Hadoop">
    <!-- Open Graph -->
    <meta property="og:title" content="[Hadoop]安装方式 - xlin的个人博客">
    <meta property="og:type" content="website">
    <meta property="og:url" content="http://localhost:4000/hadoop/2018/12/Hadoop-%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F.html">
    <meta property="og:description" content="记录编程的学习笔记">
    <meta property="og:site_name" content="xlin的个人博客">
    <link rel="stylesheet" href="//cdn.staticfile.org/normalize/6.0.0/normalize.min.css">
    <link rel="stylesheet" href="//at.alicdn.com/t/font_roc50gemkxpw4s4i.css">
    <link rel="stylesheet" href="/assets/css/github-markdown.css">
    <link rel="stylesheet" href="/assets/css/prism.css">
    <link rel="stylesheet" href="/assets/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/app.min.css">
    <link rel="stylesheet" href="/assets/css/gitalk.css">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <script src="https://cdn.staticfile.org/jquery/3.2.1/jquery.min.js"></script>
	
    <!--
Author: Ray-Eldath
-->
<style>
    .markdown-body .anchor{
        float: left;
        margin-top: -8px;
        margin-left: -20px;
        padding-right: 4px;
        line-height: 1;
        opacity: 0;
    }
    
    .markdown-body .anchor .anchor-icon{
        font-size: 15px
    }
</style>
<script>
    $(document).ready(function() {
        let nodes = document.querySelector(".markdown-body").querySelectorAll("h1,h2,h3")
        for(let node of nodes) {
            var anchor = document.createElement("a")
            var anchorIcon = document.createElement("i")
            anchorIcon.setAttribute("class", "fa fa-anchor fa-lg anchor-icon")
            anchorIcon.setAttribute("aria-hidden", true)
            anchor.setAttribute("class", "anchor")
            anchor.setAttribute("href", "#" + node.getAttribute("id"))
            
            anchor.onmouseover = function() {
                this.style.opacity = "0.4"
            }
            
            anchor.onmouseout = function() {
                this.style.opacity = "0"
            }
            
            anchor.appendChild(anchorIcon)
            node.appendChild(anchor)
        }
    })
</script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124506309-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-124506309-1');
    </script>

    
</head>


<body>
  <!--[if lt IE 10]>
<div class="alert-danger" role="alert">你的浏览器实在太太太旧了，放学别走，升级完浏览器再说！<a target="_blank" class="alert-link" href="http://browsehappy.com">立即升级</a></div>
<![endif]-->
  <input id="nm-switch" type="hidden" value="false"> <header class="g-header">
    <div class="g-logo">
      <a href="/"></a>
    </div>
    <i id="menu-toggle" class="iconfont icon-menu"></i>
    <nav class="g-nav">
        <ul>
            
            <li><a href="/">home</a></li>
            
            <li><a href="/tags.html">tags</a></li>
            
            <li><a href="/categories.html">categories</a></li>
            
        </ul>
    </nav>
</header>


  <header class="g-banner post-header post-pattern-circuitBoard bgcolor-default " data-theme="default">
    <div class="post-wrapper">
      <div class="post-tags">
        
          
            <a href="http://localhost:4000/tags#Hadoop" class="post-tag">Hadoop</a>
          
        
      </div>
      <h1>[Hadoop]安装方式</h1>
      <div class="post-meta">
        <span class="post-meta-item"><i class="iconfont icon-author"></i><a href="http://localhost:4000" target="_blank" rel="author">xlin</a></></span>
        <time class="post-meta-item" datetime="18-12-16"><i class="iconfont icon-date"></i>16 Dec 2018</time>
      </div>
    </div>
    
    <div class="filter"></div>
      <div class="post-cover" style="background: url('../../../assets/img/hadoop-logo.jpg') center no-repeat; background-size: cover;"></div>
    
  </header>

  <div class="post-content visible">
    
    <h2 class="post-subtitle">Hadoop的相关与配置信息</h2>
    

    <article class="markdown-body">
      <h2 id="下载">下载</h2>

<p> 下载地址 <code class="highlighter-rouge">archive.apache.org/dist/hadoop/common/</code> 
 相关文档<code class="highlighter-rouge">http://hadoop.apache.org/docs</code></p>

<h3 id="linux相关配置">Linux相关配置</h3>

<h5 id="1-yum源">1. yum源</h5>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. 上传包含安装包的文件到节点
2. 安装httpd包
rpm -ivh  apr-1.4.8-3.el7.x86_64.rpm 
rpm -ivh  apr-util-1.5.2-6.el7.x86_64.rpm 
rpm -ivh  httpd-tools-2.4.6-40.el7.centos.x86_64.rpm 
rpm -ivh  mailcap-2.1.41-2.el7.noarch.rpm
rpm -ivh  httpd-2.4.6-40.el7.centos.x86_64.rpm
3. 启动httpd进程，并设置为开机启动
systemctl   start    httpd
systemctl   enable   httpd
4. 安装createrepo包，用来构建软件仓库
rpm -ivh  deltarpm-3.6-3.el7.x86_64.rpm 
rpm -ivh  python-deltarpm-3.6-3.el7.x86_64.rpm 
rpm -ivh  libxml2-python-2.9.1-5.el7_1.2.x86_64.rpm 
rpm -ivh  libxml2-2.9.1-5.el7_1.2.x86_64.rpm 
rpm -ivh  createrepo-0.9.9-23.el7.noarch.rpm 
5.创建软链接到硬盘中的包文件夹
http工作目录是/var/www/html/，需要设置一个目录指向包文件夹的地址
cd  /var/www/html/
mkdir  /var/www/html/CentOS7.2/
ln -s  /data01/Packages   /var/www/html/CentOS7.2/Packages
chmod 755 -R /var/www/html/CentOS7.2/Packages
6.使用上面安装的createrepo来创建仓库
createrepo  /var/www/html/CentOS7.2/Packages
7.关闭防火墙
setenforce 0 
systemctl   stop    firewalld
访问IP地址下的/CentOS7.2/Packages能看到包并可以下载就可以了。
8.所有主机都配置一下repo文件
文件夹地址在/etc/yum.repos.d/
1.备份原来的repo源文件
cd /etc/yum.repos.d/
mkdir  bak
mv CentOS-*.repo  bak/
2.创建自己的repo文件，写入内容
vi  base.repo

[base]
name=CentOS-Packages
baseurl=http://192.168.33.201/CentOS7.2/Packages/
gpgkey=
path=/
enabled=1
gpgcheck=0
3.清理yum缓存，重新构建缓存，测试安装一个包
yum clean all
yum makecache
yum list|grep vim（举个例子，把包里面有vim的包都列出来）
</code></pre></div></div>

<h5 id="2-禁用selinux否则重启之后可能会无法访问到yum的软件库">2. 禁用SELinux，否则重启之后可能会无法访问到yum的软件库</h5>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.setenforce 0	临时关闭SELinux
2.修改/etc/selinux/config中的SELINUX=disabled
	sed -i 's/^SELINUX=.*/SELINUX=disabled/g'   /etc/selinux/config
	sed -i 's/^SELINUX=.*/SELINUX=disabled/g'   /etc/sysconfig/selinux
</code></pre></div></div>

<h5 id="3关闭thp影响hadoop集群的性能">3.关闭THP（影响hadoop集群的性能）</h5>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled
echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag
</code></pre></div></div>

<h5 id="4修改swappiness">4.修改swappiness</h5>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>这个值表示剩余多少内存的时候使用磁盘，设置为0即最大限度的使用内存
echo "vm.swappiness=0" &gt;&gt; /etc/sysctl.conf 
sysctl -p    ##让配置生效
cat /proc/sys/vm/swappiness
</code></pre></div></div>

<h5 id="5修改主机名配置主机名文件ssh免密登陆">5.修改主机名，配置主机名文件，ssh免密登陆</h5>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.修改主机名
	hostnamectl --static set-hostname hadoop1
2.配置主机名文件
	vi /etc/hosts
3.关闭防火墙即开机关闭
	systemctl stop firewalld.service
	systemctl disable firewalld.service
4.配置ssh免密登陆
	ssh-keygen
	ssh-copy-id 主机名

远程复制:
1. 复制文件（本地&gt;&gt;远程：scp /cloud/data/test.txt root@10.21.156.6:/cloud/data/
2. 复制文件（远程&gt;&gt;远程：scp root@10.21.156.6:/cloud/data/test.txt /cloud/data/
3. 复制目录（本地&gt;&gt;远程：scp -r /cloud/data root@10.21.156.6:/cloud/data/
4. 复制目录（远程&gt;&gt;本地：scp -r root@10.21.156.6:/cloud/data/ /cloud/data
</code></pre></div></div>

<h5 id="6设置时区统一时间">6.设置时区，统一时间</h5>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.所有节点设置时区：
timedatectl set-timezone "Asia/Shanghai"
2.统一时间，“对表”，即以主节点的时间为准
1.所有机器安装ntp
	yum -y install ntp
2.修改主节点配置文件，所有机器时间以主节点（如hadoop1）时间为准，
	1.所有节点备份原始配置文件
		cp /etc/ntp.conf /etc/ntp.conf.bak
	2.修改主节点hadoop1配置
		vi /etc/ntp.conf
			# server 0.centos.pool.ntp.org iburst
			# server 1.centos.pool.ntp.org iburst
			# server 2.centos.pool.ntp.org iburst
			# server 3.centos.pool.ntp.org iburst 
			server 127.127.1.1
	3.重启ntpd进程，设置开机自启
		systemctl restart ntpd
		systemctl enable ntpd
3.在其他节点上指定以hadoop1为准来进行时间校准
	ntpdate hadoop1
	systemctl start ntpd
	crontab	定时执行脚本/命令
4.修改其他节点上的配置文件（同主节点修改步骤）
	vi /etc/ntp.conf
		# server 0.centos.pool.ntp.org iburst
		# server 1.centos.pool.ntp.org iburst
		# server 2.centos.pool.ntp.org iburst
		# server 3.centos.pool.ntp.org iburst 
		server 192.168.33.201
5.其他节点上重启ntpd进程，并设置成开机自启
	systemctl restart ntpd
	systemctl enable ntpd
</code></pre></div></div>

<h2 id="hadoop相关">Hadoop相关</h2>
<ol>
  <li>安装jdk
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>JAVA_HOME
CDH中spark会默认到/usr/java/default目录下去找jdk，所以一般就安装在/usr/java目录下
export JAVA_HOME=/usr/java/jdk1.8.0_25
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$JAVA_HOME/bin:$PATH
</code></pre></div>    </div>
  </li>
  <li>解压到指定目录</li>
  <li>修改配置文件
```
1.配置jdk的地址，etc/hadoop/hadoop-env.sh文件
 # set to the root of your Java installation
 export JAVA_HOME=/usr/java/latest</li>
</ol>

<p>standalone本地运行的操作
	  $ mkdir input
	  $ cp etc/hadoop/<em>.xml input
	  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar wordcount input output
	  $ cat output/</em></p>

<p>2.配置hdfs，etc/hadoop/core-site.xml文件
新建文件夹data/tmp，用来存放临时文件</p>
<configuration>
	<property>
		<name>fs.defaultFS</name>
		## 指定了namenode的节点
		<value>hdfs://hadoop1:9001</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/modules/hadoop-2.7.7/data/tmp</value>
	</property>
</configuration>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>etc/hadoop/hdfs-site.xml文件，配置副本数
&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;1&lt;/value&gt;
	&lt;/property&gt;
	# python操作hdfs时需要获取权限
	&lt;property&gt; 
		&lt;name&gt;dfs.permissions&lt;/name&gt; 
		&lt;value&gt;false&lt;/value&gt; 
	&lt;/property&gt;
	## 指定secondarynamenode节点
	&lt;property&gt;
		&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
		&lt;value&gt;节点地址:50090&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;

3.首次启动，namenode需要格式化
	bin/hdfs namenode -format
	
hdfs的web监控端口是50070
		
4.配置yarn
etc/hadoop/mapred-site.xml文件
	&lt;configuration&gt;
		&lt;property&gt;
			&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
			&lt;value&gt;yarn&lt;/value&gt;
		&lt;/property&gt;
		## 配置historyserver
		&lt;property&gt;
			&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
			&lt;value&gt;节点主机名:10020&lt;/value&gt;
		&lt;/property&gt;
		&lt;property&gt;
			&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
			&lt;value&gt;节点主机名:19888&lt;/value&gt;
		&lt;/property&gt;
	&lt;/configuration&gt;

etc/hadoop/yarn-site.xml文件
	&lt;configuration&gt; 
    	&lt;property&gt;
    		&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    		&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    	&lt;/property&gt;
    	## 指定resourcemanager节点
    	&lt;property&gt;
    		&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    		&lt;value&gt;主节点的hostname&lt;/value&gt;
    	&lt;/property&gt;
		## 日志聚集功能开启
		&lt;property&gt;
			&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;
			&lt;value&gt;true&lt;/value&gt;
		&lt;/property&gt;
		## 日志文件保存的时间，以秒为单位
		&lt;property&gt;
			&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;
			&lt;value&gt;640800&lt;/value&gt;
		&lt;/property&gt;
	&lt;/configuration&gt;
		
slaves: 指定启动的脚本，如启动start-dfs.sh
	指定datanode和nodemanager是哪些节点
yarn的web监控端口是8088
		
5.配置文件的类别
	1.默认配置文件:四个模块相对应的jar中，$HADOOP_HOME/share/hadoop/...
		core-default.xml
		hdfs-default.xml
		yarn-default.xml
		mapred-default.xml
	2.自定义配置文件，$HADOOP_HOME/etc/hadoop/
		core-site.xml
		hdfs-site.xml
		yarn-site.xml
		mapred-site.xml
		
6.启动方式
	1.各个节点服务组件逐一启动
		hadoop-daemon.sh start namenode
		hadoop-daemon.sh start datanode
		sbin/mr-jobhistory-daemon.sh start historyserver
	2.各个模块统一启动
		hadoop1:start-dfs.sh
		hadoop2:start-yarn.sh	mr-jobhistory-daemon.sh start historyserver
			
	3.所有节点服务统一启动
		start-all.sh（不建议使用）
		因为一般实际情况下namenode和resourcemanager不建议配置在一台机器上

7.查看安全模式
	hdfs dfsadmin -safemode get
	
8.其他
	yarn默认的配置，cpu有8核，内存为8G，可以配置
		
	apache slider 动态的yarn
		把已经存在的分布式框架运行在yarn上
	
9.mr
	以wordcount为例：
	map：映射，可以高度并行
		输入输出形式都是键值对
			输入的key，value对，key为字符偏移量，value为字符串类型
			
	reduce：合并
	
	mapreduce优化
		1.reduce的个数，job.reduces 默认为1个
		2.shuffle过程的compress和combiner
		3.shuffle的各种参数调节 ```
</code></pre></div></div>

<p>上传到工作目录:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hdfs dfs -put /etc/profile /file
</code></pre></div></div>

<p><strong>参考：</strong></p>

<p><a href="https://hadoop.apache.org/docs/r2.9.2/">Hadoop官方文档</a></p>

    </article>

    
    <div class="social-share-wrapper">
      <div class="social-share"></div>
    </div>
    
  </div>

  <section class="author-detail">
    <section class="post-footer-item author-card">
      <div class="avatar">
        <img src="http://localhost:4000/assets/img/profile.png" alt="">
      </div>
      <div class="author-name" rel="author">xlin</div>
      <div class="bio">
        <p>spark/hadoop/hadoop/hbase</p>
      </div>
      
      <ul class="sns-links">
        
        <li>
          <a href="https://github.com/xlinboy" target="_blank">
                    <i class="iconfont icon-github"></i>
                </a>
        </li>
        
      </ul>
      
    </section>

    <section class="post-footer-item comment">
      <div id="comment_container"></div>
    </section>

    <section class="post-footer-item read-next">
      
      <div class="read-next-item">
        <a href="/hadoop/2018/12/MapReduce-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B.html" class="read-next-link"></a>
        <section>
          <span>[Mapreduce]执行流程</span>
          <p>  MapReduce是一个分布式运算程序的编程框架。其核心功能是：将用户编写的业务逻辑代码和自带默认组件整合成一...</p>
        </section>
        
        <div class="filter"></div>
        <img src="../../../assets/img/hadoop-logo.jpg" alt="">
        
     </div>
      

      
      <div class="read-next-item">
        <a href="/hadoop/2018/12/Hadoop-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.html" class="read-next-link"></a>
          <section>
            <span>[Hadoop]基础知识</span>
            <p>1 Hadoop</p>
          </section>
          
          <div class="filter"></div>
          <img src="../../../assets/img/hadoop-logo.jpg" alt="">
          
      </div>
      
    </section>

  </section>

  <footer class="g-footer">
  <section>xlin的个人博客 ©
  
  
    2018
    -
  
  2019
  </section>
  <section>Powered by <a href="//jekyllrb.com">Jekyll</a> | <a href="https://github.com/kaeyleo/jekyll-theme-H2O">Theme H2O</a></section>
</footer>


  <script src="/assets/js/social-share.min.js"></script>
  <script>
    socialShare('.social-share', {
      sites: [
        
          'wechat'
          ,
          
        
          'weibo'
          ,
          
        
          'twitter'
          
        
      ],
      wechatQrcodeTitle: "分享到微信朋友圈",
      wechatQrcodeHelper: '<p>扫码后点击右上角</p><p>将本文分享至朋友圈</p>'
    });
  </script>

  

  <script src="/assets/js/prism.js"></script>
  <script src="/assets/js/index.min.js"></script>
</body>

</html>
